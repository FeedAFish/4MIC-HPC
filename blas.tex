% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
  xcolor = usenames,dvipsnames]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{listings}
\newcommand{\passthrough}[1]{#1}
\lstset{defaultdialect=[5.3]Lua}
\lstset{defaultdialect=[x86masm]Assembler}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{setspace}
\usepackage{float}
\usepackage{fontspec}
\usepackage{subfig}
\setmonofont{JetBrains Mono}[Contextuals=Alternate]
\floatplacement{figure}{H}
\lstset{
  language=C,
  basicstyle={\linespread{0.8}\ttfamily},
  stepnumber=1,
  numbersep=5pt,
  backgroundcolor=\color{cyan!5},
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  frame=single,
  rulecolor=\color{black},
  tabsize=2,
  captionpos=b,
  breaklines=true,
  breakatwhitespace=false,
  keywordstyle=\color{RoyalBlue},
  commentstyle=\color{Green},
  stringstyle=\color{Orange},
  keepspaces=true
}
\makeatletter
\renewcommand\paragraph{\@startsection{paragraph}{4}{\z@}%
  {-2.5ex\@plus -1ex \@minus -.25ex}%
  {1.25ex \@plus .25ex}%
  {\normalfont\normalsize\bfseries}}
\makeatother
\setcounter{secnumdepth}{4}
\hypersetup{
  colorlinks = true,
}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\author{}
\date{\vspace{-2.5em}}

\begin{document}

\onehalfspacing

\pagenumbering{gobble}

%\begin{titlepage}
\vspace*{\fill}
\begin{center}
  \LARGE{\textbf{Optimization and OpenMP parallelization of the dense matrix-matrix product computation}}\\
  \Large{\textbf{HPC 4GMM 2021/2022}}\\
  \vspace*{1\baselineskip}
  \Large{\textbf{Members}}\\
  PHAM Tuan Kiet\\
  VO Van Nghia\\
  \vfill % equivalent to \vspace{\fill}
  \vspace*{\fill}
  \Large{\textbf{Date}}\\
  12 Dec, 2021
\end{center}
% \end{titlepage}

\newpage

\newpage
\pagenumbering{roman}
\tableofcontents
\addcontentsline{toc}{section}{\contentsname}

\newpage
\pagenumbering{arabic}

\hypertarget{techniques}{%
\section{Techniques}\label{techniques}}

\hypertarget{naive-dot}{%
\subsection{Naive dot}\label{naive-dot}}

We first mention here the original \passthrough{\lstinline!naive\_dot!} function. This function serves as an anchor (or base case) for performance comparision as well as for making sure we have the right result when using other techniques.

\begin{lstlisting}[language=C]
for (i = 0; i < M; i++)
  for (j = 0; j < N; j++)
    for (k = 0; k < K; k++) C[i + ldc * j] += A[i + lda * k] * B[k + ldb * j];
\end{lstlisting}

Below is the output of \passthrough{\lstinline!naive\_dot!} for \passthrough{\lstinline!M = 1!}, \passthrough{\lstinline!K = 2!}, \passthrough{\lstinline!N = 2!}:

\begin{lstlisting}
## 
## Parallel execution with a maximum of 4 threads callable
## 
## Scheduling static with chunk = 0
## 
## ( 1.00  1.50 )
## 
## ( 1.00  1.50 )
## ( 1.50  2.00 )
## 
## Frobenius Norm   = 5.550901
## Total time naive = 0.000001
## Gflops           = 0.008389
## 
## ( 3.25  4.50 )
\end{lstlisting}

As
\[
\begin{pmatrix}
1 & 1,5
\end{pmatrix}\begin{pmatrix}
1 & 1,5\\
1,5 & 2
\end{pmatrix}=\begin{pmatrix}
3,25 & 4,5
\end{pmatrix}
\]
The result of this function is correct. We could move on to the next technique.

\hypertarget{spatial-locality}{%
\subsection{Spatial locality}\label{spatial-locality}}

Spatial locality refers to the following scenario: if a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In order to take advantages of this property, we notice that:

\begin{itemize}
\tightlist
\item
  In memory, \passthrough{\lstinline!A!}, \passthrough{\lstinline!B!}, \passthrough{\lstinline!C!} are stored in contiguous memory block.
\item
  When using the index order \passthrough{\lstinline!i!}, \passthrough{\lstinline!j!}, \passthrough{\lstinline!k!}, we access \passthrough{\lstinline!B!} consecutively (as we access \passthrough{\lstinline!B!} by \passthrough{\lstinline!B[k + ldb * j]!}), but not \passthrough{\lstinline!A!} and \passthrough{\lstinline!C!}.
\item
  Data from \passthrough{\lstinline!A!}, \passthrough{\lstinline!B!}, \passthrough{\lstinline!C!} are loaded in a memory block consisting of severals consecutive elements to cache. Thus, we could make use of spatial locality when reading data continously.
\end{itemize}

From 3 points above, we decide to switch the index order to \passthrough{\lstinline!k!}, \passthrough{\lstinline!j!}, \passthrough{\lstinline!i!}. Now we see that both reading and writing operations on \passthrough{\lstinline!C!} are in cache, this brings us a critical gain in performance. In addition, reading operations on \passthrough{\lstinline!A!} are in cache too but those on \passthrough{\lstinline!B!} are not.

\begin{lstlisting}[language=C]
for (k = 0; k < K; k++)
  for (j = 0; j < N; j++)
    for (i = 0; i < M; i++) C[i + ldc * j] += A[i + lda * k] * B[k + ldb * j];
\end{lstlisting}

For comparision, we have a table below with small \passthrough{\lstinline!M!}, \passthrough{\lstinline!K!}, \passthrough{\lstinline!N!} (\passthrough{\lstinline!OMP!} indicates if we enable \passthrough{\lstinline!OpenMP!} or not).

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrl}
\toprule
Technique & Time ($s$) & $\|.\|_F$ & $GFlop/s$ & M & K & N & OMP\\
\midrule
naive & 0 & 3.461352 & Inf & 4 & 8 & 4 & F\\
saxpy & 0 & 3.461352 & Inf & 4 & 8 & 4 & F\\
\bottomrule
\end{tabular}}
\end{table}

We have the frobenius norm of both techniques are 3,461352, which indicate we have the right computation result. In addition, calculating time is already significantly small (\(\approx\) 0 second in both methods) and the difference between these two can therefore be ommited.

However, if we set \passthrough{\lstinline!M!}, \passthrough{\lstinline!K!}, \passthrough{\lstinline!N!} to 2048. There will be a huge performance gain as in the table shown below.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrl}
\toprule
Technique & Time ($s$) & $\|.\|_F$ & $GFlop/s$ & M & K & N & OMP\\
\midrule
naive & 62.267724 & 2.323362 & 0.275903 & 2048 & 2048 & 2048 & F\\
saxpy & 5.800958 & 2.323362 & 2.961557 & 2048 & 2048 & 2048 & F\\
\bottomrule
\end{tabular}}
\end{table}

Here, the \passthrough{\lstinline!naive\_dot!} function is approximately 11 times slower than the \passthrough{\lstinline!saxpy\_dot!} function.

\hypertarget{openmp-parallelization}{%
\subsection{OpenMP parallelization}\label{openmp-parallelization}}

For parallelism, we add a directive \passthrough{\lstinline!\#pragma omp parallel for schedule(runtime) default(shared)!} and \passthrough{\lstinline!private(i, j, k)!} depending on which variables we want to make private. A special clause \passthrough{\lstinline!reduction(+ : norm)!} is added to \passthrough{\lstinline!norm!} function as we want to sum all the \passthrough{\lstinline!norm!} from each thread to only one variable. A link to github with full source code will be provided at the end of the report. For performance comparision, we will show only one case here. More detailed study will be presented in the next sections.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrlrlr}
\toprule
Technique & Time ($s$) & $\|.\|_F$ & $GFlop/s$ & M & K & N & Block & OMP & Threads & Schedule & Chunk\\
\midrule
naive & 62.267724 & 2.323362 & 0.275903 & 2048 & 2048 & 2048 & 4 & F & 4 & static & 0\\
saxpy & 5.800958 & 2.323362 & 2.961557 & 2048 & 2048 & 2048 & 4 & F & 4 & static & 0\\
naive & 33.076697 & 2.323362 & 0.519395 & 2048 & 2048 & 2048 & 4 & T & 4 & static & 0\\
saxpy & 4.116676 & 2.323245 & 4.173238 & 2048 & 2048 & 2048 & 4 & T & 4 & static & 0\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{cache-blocking}{%
\subsection{Cache blocking}\label{cache-blocking}}

The main idea of the cache blocking technique (or tiled) is breaking the whole matrices into smaller sub-matrices so the data needed for one multiplication operation could fit into the cache, therefore leads to a much faster calculation. Furthermore, if we enable \passthrough{\lstinline!OpenMP!}, the computation would be even faster as each sub-matrice is processed by a separate thread. However, if we set \passthrough{\lstinline!BLOCK!} size too small, the benefit of dividing matrix is overshadowed by the additional loops and operations. Meanwhile, a too large \passthrough{\lstinline!BLOCK!} size leads to an overfitting (data for one operation can not be fitted into the cache), and therefore a slower operation. The principal source code is shown below:

\begin{lstlisting}[language=C]
for (k = 0; k < K; k += BLOCK) {
  for (j = 0; j < N; j += BLOCK) {
    for (i = 0; i < M; i += BLOCK) {
      int kmin = fmin(K - k, BLOCK); // in case K is not divisible by BLOCK
      for (kk = 0; kk < kmin; kk++) {
        int jmin = fmin(N - j, BLOCK); // in case N is not divisible by BLOCK
        for (jj = 0; jj < jmin; jj++) {
          int imin = fmin(M - i, BLOCK); // in case M is not divisible by BLOCK
          for (ii = 0; ii < imin; ii++) {
            C[(ii + i) + ldc * (jj + j)] +=
                A[(ii + i) + lda * (kk + k)] * B[(kk + k) + ldb * (jj + j)];
          }
        }
      }
    }
  }
}
\end{lstlisting}

In addition, we see in the table below that we have 1,71 times faster with the non \passthrough{\lstinline!OpenMP!} version and 1,79 times with \passthrough{\lstinline!OpenMP!}.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrlrlr}
\toprule
Technique & Time ($s$) & $\|.\|_F$ & $GFlop/s$ & M & K & N & Block & OMP & Threads & Schedule & Chunk\\
\midrule
naive & 62.267724 & 2.323362 & 0.275903 & 2048 & 2048 & 2048 & 256 & F & 4 & static & 0\\
saxpy & 5.800958 & 2.323362 & 2.961557 & 2048 & 2048 & 2048 & 256 & F & 4 & static & 0\\
tiled & 3.387300 & 2.323362 & 5.071848 & 2048 & 2048 & 2048 & 256 & F & 4 & static & 0\\
\addlinespace
naive & 33.076697 & 2.323362 & 0.519395 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
saxpy & 4.116676 & 2.323245 & 4.173238 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
tiled & 2.298825 & 2.321977 & 7.473326 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{blas}{%
\subsection{BLAS}\label{blas}}

One last technique that is used in our code is calling the \passthrough{\lstinline!cblas\_dgemm!} function which use the optimized BLAS implementation. This function is the fastest method even if other methods are \emph{``cheated''} (by using \passthrough{\lstinline!OpenMP!}) as their implementation is optimized based on many factors: algorithms, software and hardware.

\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrlrlr}
\toprule
Technique & Time ($s$) & $\|.\|_F$ & $GFlop/s$ & M & K & N & Block & OMP & Threads & Schedule & Chunk\\
\midrule
naive & 33.076697 & 2.323362 & 0.519395 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
saxpy & 4.116676 & 2.323245 & 4.173238 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
tiled & 2.298825 & 2.321977 & 7.473326 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
blas & 0.375638 & 2.323362 & 45.735173 & 2048 & 2048 & 2048 & 256 & T & 4 & static & 0\\
\bottomrule
\end{tabular}}
\end{table}

\hypertarget{sequential}{%
\section{Sequential}\label{sequential}}

In this section, we fix \passthrough{\lstinline!OMP\_NUM\_TREADS=1!} for each run and vary the matrix size. (We don't care about schedule because there is only 1 thread).

For the sake of simplicity, we consider the case where M and K and N are all equal and equal to a \(2^s\). The blocking size is also supposed to be in a form of \(2^t\). In addition, as \passthrough{\lstinline!OMP\_NUM\_TREADS=1!} is essentially the same logic as non \passthrough{\lstinline!OpenMP!} version, we will include a non \passthrough{\lstinline!OpenMP!} result for studying how the overhead time of \passthrough{\lstinline!OpenMP!} impact the overall performance.

In the graph below, we see that the fastest method is no doubt \passthrough{\lstinline!blas!} method, followed by \passthrough{\lstinline!tiled!} (with a block of 256). The third fastest method is \passthrough{\lstinline!saxpy!} and the slowest is \passthrough{\lstinline!naive!}. This is aligned with what we see in the \protect\hyperlink{techniques}{section 1}. In addition, the time for calculating matrices whose size is less than \(2^{10} = 1024\) is around 5 \(s\) for all methods. This could be explained by the fact that these matrices could be fitted entierly into the cache, which leads to a significant drop in computation time.

Another property that could be interesting is the version with \passthrough{\lstinline!OpenMP!} is close or even faster than the non \passthrough{\lstinline!OpenMP!} version regardless the overhead of parallelization. This could be explained by many factors \footnote{\url{https://stackoverflow.com/questions/22927973/openmp-with-single-thread-versus-without-openmp}} \footnote{\url{https://stackoverflow.com/questions/2915390/openmp-num-threads1-executes-faster-than-no-openmp}}, but the most significant one is As \passthrough{\lstinline!OpenMP!} is just API specification and C compilers are free to implement it in any way they want as long as they respect the specification, many compilers (notably modern \passthrough{\lstinline!gcc!} and \passthrough{\lstinline!clang!}) are smart enough to treat \passthrough{\lstinline!OpenMP!} version of only 1 thread the same as the sequential version. Therefore, we only see a small difference between each run. If we run both versions enough times, the average time of both will be the same.

\begin{figure}

{\centering \includegraphics{blas_files/figure-latex/unnamed-chunk-20-1} 

}

\caption{Computation time in function of matrix size and technique}\label{fig:unnamed-chunk-20}
\end{figure}

\hypertarget{threading}{%
\section{Threading}\label{threading}}

Right now, we want to see the true power of parallelism, we will fix matrix size to \passthrough{\lstinline!M!} = \passthrough{\lstinline!K!} = \passthrough{\lstinline!N!} = 2048, \passthrough{\lstinline!block!} to 256, and vary the number of threads.

We see that although the distance between are relatively small, \passthrough{\lstinline!blas!} method is still the fastest regardless the number of threads. It shows that in order to achieve high speed computation, we have to not only parallelize, but also make improvements on multiplication algorithms, memory accesses and even use assembly instructions.

In addition, the 4 schedule lines of each technique are overlapping each others and there are only very small difference in term of computational time. Except \passthrough{\lstinline!blas!} method which is not affected by the schedule options, the phenomenon happened because our problem (matrix multiplication) has a nearly the same workload at each iterations. That means the first iteration will take almost the same as the last iteration or any other iterations. For each schedule:

\begin{itemize}
\tightlist
\item
  \passthrough{\lstinline!static!} evenly-divides the total workloads into each threads, which is the best schedule for our problem.
\item
  \passthrough{\lstinline!dynamic!} and \passthrough{\lstinline!guided!} are designed for different situation, where each iteration takes different amount of time to finish their work. There is overhead compared to \passthrough{\lstinline!static!}, however, it does not have big effect on overall performance as our matrices are not too big.
\item
  \passthrough{\lstinline!auto!} lets the compiler choose how to schedule and divide work among threads, so it is compiler-specific. For example, \passthrough{\lstinline!gcc!} maps \passthrough{\lstinline!auto!} to \passthrough{\lstinline!static!} \footnote{\url{https://github.com/gcc-mirror/gcc/blob/61e53698a08dc1d9a54d785218af687a6751c1b3/libgomp/loop.c\#L195-L198}}, at a consequence, we see a similar pattern with \passthrough{\lstinline!static!}.
\end{itemize}

\begin{center}\includegraphics{blas_files/figure-latex/unnamed-chunk-22-1} \end{center}

Finally, more threads \textbf{doesn't} always mean better performance. After we increased thread to 2, time taking for one multiplication fluctuates but does not have any real decline. The reason is there are only 2 physical cores on this computer, when the number of threads goes up too high, the overhead in creating and synchronize threads will overshadowed any benefits we gain.

\hypertarget{blocking}{%
\section{Blocking}\label{blocking}}

In the last section, we will concentrate ourselves on the impact of \passthrough{\lstinline!BLOCK!} size to overall performance. We will fix \passthrough{\lstinline!M!} = \passthrough{\lstinline!K!} = \passthrough{\lstinline!N!} = 2048, number of threads to 4, \passthrough{\lstinline!static!} schedule and vary the \passthrough{\lstinline!BLOCK!} size.

\begin{center}\includegraphics{blas_files/figure-latex/unnamed-chunk-24-1} \end{center}

We see clearly that as \passthrough{\lstinline!BLOCK!} size grows, the performance becomes better but get worse after \passthrough{\lstinline!BLOCK!} size grows to approximately \(2^10\). As explained in the \protect\hyperlink{cache-blocking}{section 1}, \passthrough{\lstinline!BLOCK!} should not be too small and neither too large.

\hypertarget{conslusion}{%
\section{Conslusion}\label{conslusion}}

Throughout the report, it is shown clearly that there are many techniques to speed up our code. Depending on each problem and resources, we could also have a combination of all these techniques as well as having a detail benchmark to achive the best possible performance.

\newpage

\hypertarget{appendix-a-blas3.c}{%
\section*{\texorpdfstring{Appendix A: \texttt{blas3.c}}{Appendix A: blas3.c}}\label{appendix-a-blas3.c}}
\addcontentsline{toc}{section}{Appendix A: \texttt{blas3.c}}

We also like to include a part which details our experiences when working on this report.

\hypertarget{mistake-between-m-and-n}{%
\subsection*{\texorpdfstring{Mistake between \texttt{M} and \texttt{N}}{Mistake between M and N}}\label{mistake-between-m-and-n}}
\addcontentsline{toc}{subsection}{Mistake between \texttt{M} and \texttt{N}}

Initially, when we set \passthrough{\lstinline!M!} = 1, \passthrough{\lstinline!K!} = 2, \passthrough{\lstinline!N!} = 2, we have this output:

\begin{lstlisting}
## 
## Parallel execution with a maximum of 4 threads callable
## 
## Scheduling static with chunk = 0
## 
## ( 1.00  1.50 )
## 
## ( 1.00  1.50 )
## ( 1.50  2.00 )
## 
## Frobenius Norm   = nan
## Total time naive = 0.000000
## Gflops           = inf
## 
## ( 3.25  0.00 )
## 
## Frobenius Norm   = nan
## Total time saxpy = 0.000000
## Gflops           = inf
## 
## ( 3.25  0.00 )
## 
## Frobenius Norm   = nan
## Total time tiled = 0.000001
## Gflops           = 0.008389
## 
## ( 3.25  0.00 )
## 
## Frobenius Norm   = 3.250000
## Total time BLAS  = 0.000018
## Gflops           = 0.000447
## 
## ( 3.25  0.00 )
\end{lstlisting}

The result should be
\(\begin{pmatrix}3,25 & 4,5\end{pmatrix}\)
. Furthermore, the function signatures of \passthrough{\lstinline!norm!} and \passthrough{\lstinline!printarray!} are:

\begin{lstlisting}
double norm(int nrow, int ncol, int ld, double *A);
void print_array(int nrow, int ncol, int ld, double *A);
\end{lstlisting}

However, we found below this piece of code that suggests there maybe a mistake between \passthrough{\lstinline!M!} and \passthrough{\lstinline!N!}

\begin{lstlisting}[language=C]
printf("Frobenius Norm   = %f\n", norm(N, M, ldc, c));
// ...
print_array(M, N, ldc, c);
\end{lstlisting}

When we fixed \passthrough{\lstinline!M!} and \passthrough{\lstinline!N!} and rerun the code again. It show this output

\begin{lstlisting}
## 
## Parallel execution with a maximum of 4 threads callable
## 
## Scheduling static with chunk = 0
## 
## ( 1.00  1.50 )
## 
## ( 1.00  1.50 )
## ( 1.50  2.00 )
## 
## Frobenius Norm   = 5.550901
## Total time naive = 0.000000
## Gflops           = inf
## 
## ( 3.25  4.50 )
## 
## Frobenius Norm   = 7.155636
## Total time BLAS  = 0.000019
## Gflops           = 0.000425
## 
## ( 3.25  6.38 )
\end{lstlisting}

We succesully fixed the ``naive'' method but there are still something ``weird'' with \passthrough{\lstinline!cblas\_dgemm!} output. We dig deeper into the \href{https://www.intel.com/content/www/us/en/develop/documentation/onemkl-developer-reference-c/top/blas-and-sparse-blas-routines/blas-routines/blas-level-3-routines/cblas-gemm.html}{document} and found that there is something wrong with

\begin{lstlisting}[language=C]
int lda = N + 1;
int ldb = K + 1;
int ldc = N + 1;

double *a = (double *)malloc(lda * K * sizeof(double));
double *b = (double *)malloc(ldb * M * sizeof(double));
double *c = (double *)malloc(ldc * M * sizeof(double));

// ...

cblas_dgemm(CblasColMajor, CblasNoTrans, CblasNoTrans, M, N, K, alpha, a, lda,
            b, ldb, beta, c, ldc);
\end{lstlisting}

Here, \passthrough{\lstinline!a!} is supposed to be a \passthrough{\lstinline!double!} pointer whose size is \passthrough{\lstinline!M * K!} but the code above allocated a pointer with the size of \passthrough{\lstinline!(N + 1) * K!}. According to the document, when using with \passthrough{\lstinline!CblasColMajor!} and \passthrough{\lstinline!CblasNoTrans!}, \passthrough{\lstinline!lda!} should be the number of elements in one column, so \passthrough{\lstinline!lda!} is equal to \passthrough{\lstinline!M!}. After editing the code to

\begin{lstlisting}[language=C]
int lda = M;
int ldb = K;
int ldc = M;

double *a = (double *)malloc(lda * K * sizeof(double));
double *b = (double *)malloc(ldb * N * sizeof(double));
double *c = (double *)malloc(ldc * N * sizeof(double));
\end{lstlisting}

We got the correct result as shown in the \protect\hyperlink{techniques}{section 1}.

\hypertarget{minor-issues}{%
\subsection*{Minor issues}\label{minor-issues}}
\addcontentsline{toc}{subsection}{Minor issues}

When trying to change the schedule, the schedule value is switched. According to the \passthrough{\lstinline!OpenMP!} \href{https://www.openmp.org/spec-html/5.0/openmpsu121.html\#x158-7050003.2.12}{document}, \passthrough{\lstinline!guided!} should be 3 and \passthrough{\lstinline!auto!} is 4 instead of vice versa.

Finally, we add a

\begin{lstlisting}
free(a);
free(b);
free(c);
\end{lstlisting}

in the end for preventing memory leaks.

\hypertarget{source-code}{%
\subsection*{Source code}\label{source-code}}
\addcontentsline{toc}{subsection}{Source code}

Full source code could be found on \href{https://github.com/vnghia/4MIC-HPC}{github}.

\end{document}
