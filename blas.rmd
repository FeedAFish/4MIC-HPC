---
fontsize: 12pt
classoption: xcolor = usenames,dvipsnames
output:
  bookdown::pdf_document2:
    papersize: a4
    fig_caption: true
    highlight: tango
    keep_tex: true
    number_sections: true
    pandoc_args: --listings
    toc_depth: 3
    toc: false
    latex_engine: xelatex
    includes:
      in_header: preamble.tex
      before_body: blas_cover.tex
---

```{r include=FALSE}
library(kableExtra)
library(dplyr)
library(stringi)
library(scales)

library(ggplot2)
library(hrbrthemes)
theme_set(
    theme_ipsum(base_family = "") + 
    theme(
        axis.title.x = element_text(hjust = 0.5, size = 11),
        axis.title.y = element_text(hjust = 0.5, size = 11),
    )
)

knitr::opts_chunk$set(fig.align = "center")
options(width=80)

library(reticulate)

to_float_str <- function(num) {
  return(gsub("\\.", ",", num))
}

colnames = c("Technique", "Time ($s$)", "$\\|.\\|_F$", "$GFlop/s$", "M", "K", "N", "Block", "OMP", "Threads", "Schedule", "Chunk")
```

```{python include=FALSE}
import itertools
import os
import subprocess
import sys
from typing import List, Literal, Optional

import numpy as np
import pandas as pd

DF = None


def init_matrix(nrow: int, ncol: int, coff: float):
    m = np.empty((nrow, ncol))
    for i in range(nrow):
        for j in range(ncol):
            m[i, j] = coff * (i + 1 + j + 1) / nrow / ncol
    return m


def pmatrix(a: np.ndarray):
    if len(a.shape) != 2:
        raise ValueError("pmatrix can only display two dimensions")
    latex = r"\begin{pmatrix}" + "\n"
    nrow, ncol = a.shape
    for i in range(nrow):
        for j in range(ncol):
            fraction, integer = np.modf(a[i, j])
            if fraction == 0:
                latex += str(int(integer))
            else:
                latex += str(a[i, j]).replace(".", ",")
            if j != ncol - 1:
                latex += " & "
        if i != nrow - 1:
            latex += r"\\" + "\n"
    latex += "\n" + r"\end{pmatrix}"
    return latex


def get_database():
    global DF
    if DF is None:
        if os.path.exists(".local.runtime.csv"):
            DF = pd.read_csv(".local.runtime.csv")
        else:
            DF = pd.DataFrame(
                [
                    {
                        "Technique": "naive",
                        "Time": 0,
                        "Norm": 0,
                        "Gflops": 0,
                        "M": 0,
                        "K": 0,
                        "N": 0,
                        "Block": 0,
                        "OMP": False,
                        "Threads": 0,
                        "Schedule": "static",
                        "Chunk": 0,
                    }
                ]
            )
    return DF


def insert_database(items: pd.DataFrame):
    global DF
    DF = pd.concat([DF, items], ignore_index=True)


def save_database():
    global DF
    DF.to_csv(".local.runtime.csv", index=False)


def compile_raw_output(
    M: int = 4,
    K: int = 8,
    N: int = 4,
    block: int = 4,
    omp: bool = False,
    num_threads: Optional[int] = None,
    schedule: Optional[Literal["static", "dynamic", "guided", "auto"]] = None,
    chunk: Optional[int] = None,
    naive: bool = False,
    saxpy: bool = False,
    tiled: bool = False,
    blas: bool = False,
    printarray: bool = False,
    sourcepath: str = "blas3.c",
    executable: str = "blas3",
):
    command = "clang" if sys.platform == "darwin" else "gcc"
    args = [command, "-o", executable, sourcepath, "-O3", "-lblas", "-fopenmp", "-lm"]

    args.append(f"-D M_HPC={M}")
    args.append(f"-D K_HPC={K}")
    args.append(f"-D N_HPC={N}")
    args.append(f"-D BLOCK_HPC={block}")

    if not omp:
        args.append("-D NO_OMP")

    if not naive:
        args.append("-D NO_NAIVE_DOT")
    if not saxpy:
        args.append("-D NO_SAXPY_DOT")
    if not tiled:
        args.append("-D NO_BLOCKING_DOT")
    if not blas:
        args.append("-D NO_BLAS_DOT")

    if not printarray:
        args.append("-D NO_PRINT_ARRAY")

    if sys.platform == "darwin":
        args += [
            "-I/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/System/Library/Frameworks/Accelerate.framework/Versions/Current/Frameworks/vecLib.framework/Headers/",
            "-L/usr/local/Cellar/llvm/13.0.0_2/lib/",
        ]

    subprocess.run([" ".join(args)], shell=True)

    run_envs = os.environ.copy()
    if num_threads:
        run_envs["OMP_NUM_THREADS"] = str(num_threads)
    if schedule:
        run_envs["OMP_SCHEDULE"] = f"{schedule},{chunk}"
    return (
        subprocess.run(f"./{executable}", capture_output=True, env=run_envs)
        .stdout.decode("utf-8")
        .splitlines()
    )


def parse_schedule_and_num_threads(outputs: List[str]):
    num_threads = 0
    schedule: Literal["static", "dynamic", "guided", "auto"] = "static"
    chunk = 0
    for output in outputs:
        if output.startswith("Parallel execution with a maximum of "):
            raw_num_threads = output[len("Parallel execution with a maximum of ") :]
            num_threads = int(
                "".join(itertools.takewhile(str.isdigit, raw_num_threads))
            )
        elif output.startswith("Scheduling "):
            lhs, rhs = output.split(" = ")
            raw_schedule = lhs[len("Scheduling ") :]
            schedule = "".join(itertools.takewhile(str.isalpha, raw_schedule))  # type: ignore
            chunk = int(rhs)
    return num_threads, schedule, chunk


def parse_computation_output(
    outputs: List[str],
    M: int,
    K: int,
    N: int,
    block: int,
    omp: bool,
    num_threads: int,
    schedule: Literal["static", "dynamic", "guided", "auto"],
    chunk: int,
):
    result = []
    i = 0
    while i < len(outputs):
        output = outputs[i].strip()
        if output.startswith("Total time "):
            lhs, rhs = output.split(" = ")
            name: Literal["naive", "saxpy", "tiled", "blas"] = lhs[len("Total time ") :].strip().lower()  # type: ignore
            time = float(rhs)
            lhs, rhs = outputs[i - 1].strip().split(" = ")
            norm = float(rhs)
            lhs, rhs = outputs[i + 1].strip().split(" = ")
            gflops = float(rhs)
            result.append(
                {
                    "Technique": name,
                    "Time": time,
                    "Norm": norm,
                    "Gflops": gflops,
                    "M": M,
                    "K": K,
                    "N": N,
                    "Block": block,
                    "OMP": omp,
                    "Threads": num_threads,
                    "Schedule": schedule,
                    "Chunk": chunk,
                }
            )
            i = i + 1
        i = i + 1
    return result


def get_default_omp_options():
    outputs = compile_raw_output()
    return parse_schedule_and_num_threads(outputs)


def query(
    names: List[Literal["naive", "saxpy", "tiled", "blas"]],
    db: pd.DataFrame,
    M: int,
    K: int,
    N: int,
    block: Optional[int],
    omp: Optional[bool],
    num_threads: Optional[int],
    schedule: Optional[Literal["static", "dynamic", "guided", "auto"]],
    chunk: Optional[int],
):
    conditions = (db.M == M) & (db.K == K) & (db.N == N)
    conditions = conditions if block is None else conditions & (db.Block == block)
    conditions = conditions if omp is None else conditions & (db.OMP == omp)
    conditions = (
        conditions if num_threads is None else conditions & (db.Threads == num_threads)
    )
    conditions = (
        conditions if schedule is None else conditions & (db.Schedule == schedule)
    )
    conditions = conditions if chunk is None else conditions & (db.Chunk == chunk)
    conditions &= db.Technique.isin(names)

    return db[conditions]


def compile_and_run(
    M: int = 4,
    K: int = 8,
    N: int = 4,
    block: int = 4,
    omp: bool = False,
    num_threads: Optional[int] = None,
    schedule: Optional[Literal["static", "dynamic", "guided", "auto"]] = None,
    chunk: Optional[int] = None,
    naive: bool = False,
    saxpy: bool = False,
    tiled: bool = False,
    blas: bool = False,
    printarray: bool = False,
):
    default_num_threads, default_schedule, default_chunk = get_default_omp_options()
    num_threads = num_threads if num_threads else default_num_threads
    schedule = schedule if schedule else default_schedule
    chunk = chunk if chunk else default_chunk

    logpath = f".local.{M}.{K}.{N}.{block}.{omp}.{num_threads}.{schedule}.{chunk}.{naive}.{saxpy}.{tiled}.{blas}.csv"
    if os.path.exists(logpath):
        return pd.read_csv(logpath)

    names: List[Literal["naive", "saxpy", "tiled", "blas"]] = []
    if naive:
        names.append("naive")
    if saxpy:
        names.append("saxpy")
    if tiled:
        names.append("tiled")
    if blas:
        names.append("blas")

    db = get_database()

    items = query(["naive"], db, M, K, N, None, omp, num_threads, schedule, chunk)
    if not items.empty:
        if naive:
            if (items.Block != block).all():
                res = pd.DataFrame(items.tail(1)).reset_index(drop=True)
                res.loc[0, "Block"] = block
                insert_database(res)
        naive = False

    items = query(["saxpy"], db, M, K, N, None, omp, num_threads, schedule, chunk)
    if not items.empty:
        if saxpy:
            if (items.Block != block).all():
                res = pd.DataFrame(items.tail(1)).reset_index(drop=True)
                res.loc[0, "Block"] = block
                insert_database(res)
        saxpy = False

    items = query(["tiled"], db, M, K, N, block, omp, num_threads, schedule, chunk)
    if not items.empty:
        tiled = False

    items = query(["blas"], db, M, K, N, None, None, None, None, None)
    if not query(["blas"], db, M, K, N, None, None, None, None, None).empty:
        if blas:
            res = pd.DataFrame(items.tail(1)).reset_index(drop=True)
            insert = query(
                ["blas"], db, M, K, N, block, omp, num_threads, schedule, chunk
            ).empty
            if insert:
                res.loc[0, "Block"] = block
                res.loc[0, "OMP"] = omp
                res.loc[0, "Threads"] = num_threads
                res.loc[0, "Schedule"] = schedule
                res.loc[0, "Chunk"] = chunk
                insert_database(res)
        blas = False

    if naive or saxpy or tiled or blas:
        outputs = compile_raw_output(
            M,
            K,
            N,
            block,
            omp,
            num_threads,
            schedule,
            chunk,
            naive,
            saxpy,
            tiled,
            blas,
            printarray,
        )
        results = pd.DataFrame(
            parse_computation_output(
                outputs, M, K, N, block, omp, num_threads, schedule, chunk
            )
        )
        insert_database(results)
        save_database()

    db = get_database()
    df = pd.DataFrame(
        query(names, db, M, K, N, block, omp, num_threads, schedule, chunk)
    ).reset_index(drop=True)
    df.Technique = pd.Categorical(df.Technique, ["naive", "saxpy", "tiled", "blas"])
    df.sort_values(["M", "K", "N", "OMP", "Block", "Technique"])
    df.to_csv(logpath, index=False)
    return df

```

# Techniques {#techniques}
## Naive dot

We first mention here the original `naive_dot` function. This function serves as an anchor (or base case) for performance comparision as well as for making sure we have the right result when using other techniques.

```{c, eval=FALSE}
for (i = 0; i < M; i++)
  for (j = 0; j < N; j++)
    for (k = 0; k < K; k++) C[i + ldc * j] += A[i + lda * k] * B[k + ldb * j];
```

Below is the output of `naive_dot` for `M = 1`, `K = 2`, `N = 2`:
```{python echo=FALSE}
print(
  "\n".join(compile_raw_output(
    1, 2, 2, omp=False, printarray=True, naive=True
  ))
)
```
As
$$
```{python results="asis", echo=FALSE}
naive_a = init_matrix(1, 2, 1)
naive_b = init_matrix(2, 2, 2)
print(pmatrix(naive_a) + pmatrix(naive_b) + "=" + pmatrix(naive_a @ naive_b))
```
$$
The result of this function is correct. We could move on to the next technique.

## Spatial locality

Spatial locality refers to the following scenario: if a particular storage location is referenced at a particular time, then it is likely that nearby memory locations will be referenced in the near future. In order to take advantages of this property, we notice that:

  - In memory, `A`, `B`, `C` are stored in contiguous memory block.
  - When using the index order `i`, `j`, `k`, we access `B` consecutively (as we access `B` by `B[k + ldb * j]`), but not `A` and `C`.
  - Data from `A`, `B`, `C` are loaded in a memory block consisting of severals consecutive elements to cache. Thus, we could make use of spatial locality when reading data continously.

From 3 points above, we decide to switch the index order to `k`, `j`, `i`. Now we see that both reading and writing operations on `C` are in cache, this brings us a critical gain in performance. In addition, reading operations on `A` are in cache too but those on `B` are not.
```{c, eval=FALSE}
for (k = 0; k < K; k++)
  for (j = 0; j < N; j++)
    for (i = 0; i < M; i++) C[i + ldc * j] += A[i + lda * k] * B[k + ldb * j];
```

For comparision, we have a table below with small `M`, `K`, `N` (`OMP` indicates if we enable `OpenMP` or not).
```{python include=FALSE}
small_naive_saxpy_df = compile_and_run(
  4, 8, 4, omp=False, printarray=True, naive=True, saxpy=True
)
```
```{r echo=FALSE}
py$small_naive_saxpy_df %>%
  select(-c("Block", "Threads", "Schedule", "Chunk")) %>%
  mutate(OMP = ifelse(OMP, "T", "F")) %>%
  kbl(
    booktabs = T,
    format.args = list(scientific = FALSE),
    col.names = c(colnames[1:7], "OMP"),
    escape = FALSE
  ) %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

We have the frobenius norm of both techniques are `r to_float_str(py$small_naive_saxpy_df$Norm[1])`, which indicate we have the right computation result. In addition, calculating time is already significantly small ($\approx$ 0 second in both methods) and the difference between these two can therefore be ommited.

However, if we set `M`, `K`, `N` to 2048. There will be a huge performance gain as in the table shown below.
```{python include=FALSE}
big_naive_saxpy_df = compile_and_run(
  2048, 2048, 2048, omp=False, naive=True, saxpy=True
)
```
```{r echo=FALSE}
py$big_naive_saxpy_df %>%
  select(-c("Block", "Threads", "Schedule", "Chunk")) %>%
  mutate(OMP = ifelse(OMP, "T", "F")) %>%
  kbl(
    booktabs = T,
    format.args = list(scientific = FALSE),
    col.names = c(colnames[1:7], "OMP"),
    escape = FALSE
  ) %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

Here, the `naive_dot` function is approximately `r round(py$big_naive_saxpy_df$Time[1]/py$big_naive_saxpy_df$Time[2])` times slower than the `saxpy_dot` function.

## OpenMP parallelization

For parallelism, we add a directive `#pragma omp parallel for schedule(runtime) default(shared)` and `private(i, j, k)` depending on which variables we want to make private. A special clause `reduction(+ : norm)` is added to `norm` function as we want to sum all the `norm` from each thread to only one variable. A link to github with full source code will be provided at the end of the report. For performance comparision, we will show only one case here. More detailed study will be presented in the next sections.

```{python include=FALSE}
big_omp_naive_saxpy_df = compile_and_run(
  2048, 2048, 2048, omp=True, naive=True, saxpy=True
)
```
```{r echo=FALSE}
rbind(py$big_naive_saxpy_df, py$big_omp_naive_saxpy_df) %>%
  mutate(OMP = ifelse(OMP, "T", "F")) %>%
  kbl(
    booktabs = T,
    format.args = list(scientific = FALSE),
    col.names = colnames,
    escape=FALSE
  ) %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

## Cache blocking {#cache-blocking}

The main idea of the cache blocking technique (or tiled) is breaking the whole matrices into smaller sub-matrices so the data needed for one multiplication operation could fit into the cache, therefore leads to a much faster calculation. Furthermore, if we enable `OpenMP`, the computation would be even faster as each sub-matrice is processed by a separate thread. However, if we set `BLOCK` size too small, the benefit of dividing matrix is overshadowed by the additional loops and operations. Meanwhile, a too large `BLOCK` size leads to an overfitting (data for one operation can not be fitted into the cache), and therefore a slower operation. The principal source code is shown below:

```{c eval=FALSE}
for (k = 0; k < K; k += BLOCK) {
  for (j = 0; j < N; j += BLOCK) {
    for (i = 0; i < M; i += BLOCK) {
      int kmin = fmin(K - k, BLOCK); // in case K is not divisible by BLOCK
      for (kk = 0; kk < kmin; kk++) {
        int jmin = fmin(N - j, BLOCK); // in case N is not divisible by BLOCK
        for (jj = 0; jj < jmin; jj++) {
          int imin = fmin(M - i, BLOCK); // in case M is not divisible by BLOCK
          for (ii = 0; ii < imin; ii++) {
            C[(ii + i) + ldc * (jj + j)] +=
                A[(ii + i) + lda * (kk + k)] * B[(kk + k) + ldb * (jj + j)];
          }
        }
      }
    }
  }
}
```
```{python include=FALSE}
blocking_df = compile_and_run(
  2048, 2048, 2048, 256, omp=False, naive=True, saxpy=True, tiled=True,
)
blocking_omp_df = compile_and_run(
  2048, 2048, 2048, 256, omp=True, naive=True, saxpy=True, tiled=True,
)
```

In addition, we see in the table below that we have `r to_float_str(round(py$blocking_df$Time[2]/py$blocking_df$Time[3], 2))` times faster with the non `OpenMP` version and `r to_float_str(round(py$blocking_omp_df$Time[2]/py$blocking_omp_df$Time[3], 2))` times with `OpenMP`.

```{r echo=FALSE}
rbind(py$blocking_df, py$blocking_omp_df) %>%
  mutate(OMP = ifelse(OMP, "T", "F")) %>%
  kbl(
    booktabs = T,
    format.args = list(scientific = FALSE),
    linesep = c("", "", "\\addlinespace"),
    col.names = colnames,
    escape = FALSE
  ) %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

## BLAS

One last technique that is used in our code is calling the `cblas_dgemm` function which use the optimized BLAS implementation. This function is the fastest method even if other methods are *"cheated"* (by using `OpenMP`)  as their implementation is optimized based on many factors: algorithms, software and hardware.
```{python include=FALSE}
all_df = compile_and_run(
  2048, 2048, 2048, 256, omp=True, naive=True, saxpy=True, tiled=True, blas=True,
)
```
```{r echo=FALSE}
py$all_df %>%
  mutate(OMP = ifelse(OMP, "T", "F")) %>%
  kbl(
    booktabs = T,
    format.args = list(scientific = FALSE),
    col.names = colnames,
    escape = FALSE
  ) %>% 
  kable_styling(latex_options = c("hold_position", "scale_down"))
```

# Sequential

In this section, we fix `OMP_NUM_TREADS=1` for each run and vary the matrix size. (We don't care about schedule because there is only 1 thread).

For the sake of simplicity, we consider the case where M and K and N are all equal and equal to a $2^s$. The blocking size is also supposed to be in a form of $2^t$. In addition, as `OMP_NUM_TREADS=1` is essentially the same logic as non `OpenMP` version, we will include a non `OpenMP` result for studying how the overhead time of `OpenMP` impact the overall performance.

```{python include=FALSE}
omp_df = None
omp_df_path = ".local.omp.df.csv"
non_omp_df = None
non_omp_df_path = ".local.non.omp.df.csv"
if os.path.exists(omp_df_path) and os.path.exists(non_omp_df_path):
    omp_df = pd.read_csv(omp_df_path)
    non_omp_df = pd.read_csv(non_omp_df_path)
else:
    omp_dfs = []
    non_omp_dfs = []
    for s in range(1, 13):
        for t in range(0, 11):
            M = N = K = 2 ** s
            block = 2 ** t
            omp_dfs.append(
                compile_and_run(
                    M,
                    N,
                    K,
                    block,
                    True,
                    1,
                    naive=True,
                    saxpy=True,
                    tiled=True,
                    blas=True,
                )
            )
            non_omp_dfs.append(
                compile_and_run(
                    M,
                    N,
                    K,
                    block,
                    False,
                    1,
                    naive=True,
                    saxpy=True,
                    tiled=True,
                    blas=True,
                )
            )
    omp_df = pd.concat(omp_dfs, ignore_index=True)
    omp_df.to_csv(omp_df_path, index=False)
    non_omp_df = pd.concat(non_omp_dfs, ignore_index=True)
    non_omp_df.to_csv(non_omp_df_path, index=False)

omp_df.Technique = pd.Categorical(omp_df.Technique, ["naive", "saxpy", "tiled", "blas"])
non_omp_df.Technique = pd.Categorical(non_omp_df.Technique, ["naive", "saxpy", "tiled", "blas"])

```

In the graph below, we see that the fastest method is no doubt `blas` method, followed by `tiled` (with a block of 256). The third fastest method is `saxpy` and the slowest is `naive`. This is aligned with what we see in the [section 1](#techniques). In addition, the time for calculating matrices whose size is less than $2^{10} = 1024$ is around 5 $s$ for all methods. This could be explained by the fact that these matrices could be fitted entierly into the cache, which leads to a significant drop in computation time.

Another property that could be interesting is the version with `OpenMP` is close or even faster than the non `OpenMP` version regardless the overhead of parallelization. This could be explained by many factors ^[https://stackoverflow.com/questions/22927973/openmp-with-single-thread-versus-without-openmp] ^[https://stackoverflow.com/questions/2915390/openmp-num-threads1-executes-faster-than-no-openmp], but the most significant one is As `OpenMP` is just API specification and C compilers are free to implement it in any way they want as long as they respect the specification, many compilers (notably modern `gcc` and `clang`) are smart enough to treat `OpenMP` version of only 1 thread the same as the sequential version. Therefore, we only see a small difference between each run. If we run both versions enough times, the average time of both will be the same.

```{r echo=FALSE}
part2 = rbind(py$omp_df, py$non_omp_df)
```
```{r echo=FALSE, fig.cap="Computation time in function of matrix size and technique"}
part2[part2$Block == 256,] %>%
    ggplot(aes(x = M, y = Time, shape = OMP, color = Technique, group = interaction(OMP, Technique))) +
    geom_line(alpha = 0.75) +
    geom_point(size = 2, alpha = 0.9) +
    scale_x_continuous(
      trans = log2_trans(),
      breaks = trans_breaks("log2", function(x) 2^x),
      labels = trans_format("log2", math_format(2^.x))
    ) +
    scale_y_continuous(
      breaks = breaks_width(300),
      minor_breaks = breaks_width(60),
    ) +
    xlab("M = K = N") +
    ylab("Time (s)") +
    theme(legend.position="bottom") +
    scale_color_manual(values = rainbow(4))
```

# Threading

Right now, we want to see the true power of parallelism, we will fix matrix size to `M` = `K` = `N` = 2048, `block` to 256, and vary the number of threads.

```{python include=FALSE}
thread_df = None
thread_df_path = ".local.thread.df.csv"
if os.path.exists(thread_df_path):
    thread_df = pd.read_csv(thread_df_path)
else:
    thread_dfs = []
    for i in range(0, 8):
        num_threads = 2 ** i
        thread_dfs.append(
            compile_and_run(
                2048,
                2048,
                2048,
                256,
                True,
                num_threads,
                "static",
                0,
                naive=True,
                saxpy=True,
                tiled=True,
                blas=True,
            )
        )
        thread_dfs.append(
            compile_and_run(
                2048,
                2048,
                2048,
                256,
                True,
                num_threads,
                "dynamic",
                1,
                naive=True,
                saxpy=True,
                tiled=True,
                blas=True,
            )
        )
        thread_dfs.append(
            compile_and_run(
                2048,
                2048,
                2048,
                256,
                True,
                num_threads,
                "guided",
                1,
                naive=True,
                saxpy=True,
                tiled=True,
                blas=True,
            )
        )
        thread_dfs.append(
            compile_and_run(
                2048,
                2048,
                2048,
                256,
                True,
                num_threads,
                "auto",
                1,
                naive=True,
                saxpy=True,
                tiled=True,
                blas=True,
            )
        )
    thread_df = pd.concat(thread_dfs, ignore_index=True)
    thread_df.to_csv(thread_df_path, index=False)

thread_df.Technique = pd.Categorical(
    thread_df.Technique, ["naive", "saxpy", "tiled", "blas"]
)
thread_df.Schedule = pd.Categorical(
    thread_df.Schedule, ["static", "dynamic", "guided", "auto"]
)

```

We see that although the distance between are relatively small, `blas` method is still the fastest regardless the number of threads. It shows that in order to achieve high speed computation, we have to not only parallelize, but also make improvements on multiplication algorithms, memory accesses and even use assembly instructions.

In addition, the 4 schedule lines of each technique are overlapping each others and there are only very small difference in term of computational time. Except `blas` method which is not affected by the schedule options, the phenomenon happened because our problem (matrix multiplication) has a nearly the same workload at each iterations. That means the first iteration will take almost the same as the last iteration or any other iterations. For each schedule:

- `static` evenly-divides the total workloads into each threads, which is the best schedule for our problem.
-  `dynamic` and `guided` are designed for different situation, where each iteration takes different amount of time to finish their work. There is overhead compared to `static`, however, it does not have big effect on overall performance as our matrices are not too big.
- `auto` lets the compiler choose how to schedule and divide work among threads, so it is compiler-specific. For example, `gcc` maps `auto` to `static` ^[https://github.com/gcc-mirror/gcc/blob/61e53698a08dc1d9a54d785218af687a6751c1b3/libgomp/loop.c#L195-L198], at a consequence, we see a similar pattern with `static`.

```{r echo=FALSE}
part3 = py$thread_df
part3 %>%
    ggplot(aes(x = Threads, y = Time, shape = Schedule, color = Technique, group = interaction(Schedule, Technique))) +
    geom_line(alpha = 0.75) +
    geom_point(size = 2, alpha = 0.9) +
    scale_x_continuous(
      trans = log2_trans(),
      breaks = trans_breaks("log2", function(x) 2^x),
      labels = trans_format("log2", math_format(2^.x))
    ) +
    xlab("Number of threads") +
    ylab("Time (s)") +
    theme(legend.position="bottom", legend.box = "vertical") +
    scale_color_manual(values = rainbow(4))
```

Finally, more threads **doesn't** always mean better performance. After we increased thread to 2, time taking for one multiplication fluctuates but does not have any real decline. The reason is there are only 2 physical cores on this computer, when the number of threads goes up too high, the overhead in creating and synchronize threads will overshadowed any benefits we gain.

# Blocking

In the last section, we will concentrate ourselves on the impact of `BLOCK` size to overall performance. We will fix `M` = `K` = `N` = 2048, number of threads to 4, `static` schedule and vary the `BLOCK` size.

```{python include=FALSE}
block_df = None
block_df_path = ".local.block.df.csv"
if os.path.exists(block_df_path):
    block_df = pd.read_csv(block_df_path)
else:
    block_dfs = []
    for i in range(0, 12):
        block = 2 ** i
        block_dfs.append(
            compile_and_run(
                4096,
                4096,
                4096,
                block,
                True,
                4,
                "static",
                0,
                saxpy=True,
                tiled=True,
                blas=True,
            )
        )
        block_dfs.append(
            compile_and_run(
                4096,
                4096,
                4096,
                block,
                False,
                4,
                "static",
                0,
                saxpy=True,
                tiled=True,
                blas=True,
            )
        )
    block_df = pd.concat(block_dfs, ignore_index=True)
    block_df.to_csv(block_df_path, index=False)

block_df.Technique = pd.Categorical(
    block_df.Technique, ["naive", "saxpy", "tiled", "blas"]
)

```

```{r echo=FALSE}
part4 = py$block_df
part4 %>%
    ggplot(aes(x = Block, y = Time, shape = OMP, color = Technique, group = interaction(OMP, Technique))) +
    geom_line(alpha = 0.75) +
    geom_point(size = 2, alpha = 0.9) +
    scale_x_continuous(
      trans = log2_trans(),
      breaks = trans_breaks("log2", function(x) 2^x),
      labels = trans_format("log2", math_format(2^.x))
    ) +
    xlab("Block size") +
    ylab("Time (s)") +
    theme(legend.position="bottom") +
    scale_color_manual(values = rainbow(4)[2:5])
```

We see clearly that as `BLOCK` size grows, the performance becomes better but get worse after `BLOCK` size grows to approximately $2^10$. As explained in the [section 1](#cache-blocking), `BLOCK` should not be too small and neither too large.

# Conslusion

Throughout the report, it is shown clearly that there are many techniques to speed up our code. Depending on each problem and resources, we could also have a combination of all these techniques as well as having a detail benchmark to achive the best possible performance.

\newpage

# Appendix A: `blas3.c` {-}

We also like to include a part which details our experiences when working on this report.

## Mistake between `M` and `N` {-}

Initially, when we set `M` = 1, `K` = 2, `N` = 2, we have this output:
```{python echo=FALSE}
print(
  "\n".join(compile_raw_output(
    1, 2, 2, omp=False, printarray=True, naive=True, saxpy=True, tiled=True, blas=True, sourcepath="blas3_moodle.c", executable="blas3_moodle"
  ))
)
```

The result should be
```{python results="asis", echo=FALSE}
print("\\(" + pmatrix(naive_a @ naive_b).replace("\n", "") + "\\)", end="")
```
. Furthermore, the function signatures of `norm` and `printarray` are:
```
double norm(int nrow, int ncol, int ld, double *A);
void print_array(int nrow, int ncol, int ld, double *A);
```
However, we found below this piece of code that suggests there maybe a mistake between `M` and `N`
```
printf("Frobenius Norm   = %f\n", norm(N, M, ldc, c));
// ...
print_array(M, N, ldc, c);
```
